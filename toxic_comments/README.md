# Обнаружение токсичных комментариев - описание проекта


## Данные

В наличии был набор размеченных комментариев. Столбец text содержит текст комментария, а toxic — целевой признак (1 - комментарий токсичный, 0 - нет).

## Задача

Построить модель, которая будет выявлять токсичные комментарии и отправлять их на модерацию. Метрики качества модели F1 должна получиться не меньше 0.75.

## Используемые библиотеки

*pandas* *numpy* *nltk* *sklearn* *lightgbm* tqdm*

### Дополнительно для BERT

*pytorch* *transformers*
